% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{memory}
\BIBentryALTinterwordspacing
B.~Zhang. A solution to the memory limit challenge in big data machine
  learning. [Online]. Available:
  \url{https://medium.com/@Petuum/a-solution-to-the-memory-limit-challenge-in-big-data-machine-learning-49783a72088b}
\BIBentrySTDinterwordspacing

\bibitem{abadi2016tensorflow}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard \emph{et~al.}, ``Tensorflow: A system for
  large-scale machine learning,'' in \emph{12th $\{$USENIX$\}$ Symposium on
  Operating Systems Design and Implementation ($\{$OSDI$\}$ 16)}, 2016, pp.
  265--283.

\bibitem{sergeev2018horovod}
A.~Sergeev and M.~Del~Balso, ``Horovod: fast and easy distributed deep learning
  in tensorflow,'' \emph{arXiv preprint arXiv:1802.05799}, 2018.

\bibitem{challengings}
\BIBentryALTinterwordspacing
H.~Zhang, \emph{Intro to Distributed Deep Learning Systems}. [Online].
  Available:
  \url{https://medium.com/@Petuum/intro-to-distributed-deep-learning-systems-a2e45c6b8e7}
\BIBentrySTDinterwordspacing

\bibitem{ben2018demystifying}
T.~Ben-Nun and T.~Hoefler, ``Demystifying parallel and distributed deep
  learning: An in-depth concurrency analysis,'' \emph{arXiv preprint
  arXiv:1802.09941}, 2018.

\bibitem{chen2016revisiting}
J.~Chen, X.~Pan, R.~Monga, S.~Bengio, and R.~Jozefowicz, ``Revisiting
  distributed synchronous sgd,'' \emph{arXiv preprint arXiv:1604.00981}, 2016.

\bibitem{wiki:ANN}
\BIBentryALTinterwordspacing
{Wikipedia contributors}, ``Artificial neural network --- {W}ikipedia{,} the
  free encyclopedia,'' 2018, [Online; accessed 01-Dec-2018]. [Online].
  Available: \url{https://en.wikipedia.org/wiki/Artificial_neural_network}
\BIBentrySTDinterwordspacing

\bibitem{Loss}
C.~M.~B. et~al, \emph{Pattern Recognition and Machine Learning}.\hskip 1em plus
  0.5em minus 0.4em\relax Springer, 2006.

\bibitem{rumelhart:errorpropnonote}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams, ``Learning internal
  representations by error propagation,'' in \emph{Parallel Distributed
  Processing: Explorations in the Microstructure of Cognition, {V}olume 1:
  {F}oundations}, D.~E. Rumelhart and J.~L. Mcclelland, Eds.\hskip 1em plus
  0.5em minus 0.4em\relax Cambridge, MA: MIT Press, 1986, pp. 318--362.

\bibitem{li2015malt}
H.~Li, A.~Kadav, E.~Kruus, and C.~Ungureanu, ``Malt: distributed
  data-parallelism for existing ml applications,'' in \emph{Proceedings of the
  Tenth European Conference on Computer Systems}.\hskip 1em plus 0.5em minus
  0.4em\relax ACM, 2015, p.~3.

\bibitem{SGD}
L.~Bottou, ``Large-scale machine learning with stochastic gradient descent,''
  in \emph{Proceedings of COMPSTAT'2010}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2010, pp. 177--186.

\bibitem{zhang2017parameter}
J.~Zhang, H.~Tu, Y.~Ren, J.~Wan, L.~Zhou, M.~Li, J.~Wang, L.~Yu, C.~Zhao, and
  L.~Zhang, ``A parameter communication optimization strategy for distributed
  machine learning in sensors,'' \emph{Sensors}, vol.~17, no.~10, p. 2172,
  2017.

\bibitem{BSP}
L.~G. Valiant, ``A bridging model for parallel computation,''
  \emph{Communications of the ACM}, vol.~33, no.~8, pp. 103--111, 1990.

\bibitem{xing2016strategies}
E.~P. Xing, Q.~Ho, P.~Xie, and D.~Wei, ``Strategies and principles of
  distributed machine learning on big data,'' \emph{Engineering}, vol.~2,
  no.~2, pp. 179--195, 2016.

\bibitem{ho2013more}
Q.~Ho, J.~Cipar, H.~Cui, S.~Lee, J.~K. Kim, P.~B. Gibbons, G.~A. Gibson,
  G.~Ganger, and E.~P. Xing, ``More effective distributed ml via a stale
  synchronous parallel parameter server,'' in \emph{Advances in neural
  information processing systems}, 2013, pp. 1223--1231.

\bibitem{KUO1}
K.~Zhang, S.~Alqahtani, and M.~Demirbas, ``A comparison of distributed machine
  learning platforms,'' in \emph{Computer Communication and Networks (ICCCN),
  2017 26th International Conference on}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2017, pp. 1--9.

\bibitem{smola2010architecture}
A.~Smola and S.~Narayanamurthy, ``An architecture for parallel topic models,''
  \emph{Proceedings of the VLDB Endowment}, vol.~3, no. 1-2, pp. 703--710,
  2010.

\bibitem{dean2012large}
J.~Dean, G.~Corrado, R.~Monga, K.~Chen, M.~Devin, M.~Mao, A.~Senior, P.~Tucker,
  K.~Yang, Q.~V. Le \emph{et~al.}, ``Large scale distributed deep networks,''
  in \emph{Advances in neural information processing systems}, 2012, pp.
  1223--1231.

\bibitem{li2014scaling}
M.~Li, D.~G. Andersen, J.~W. Park, A.~J. Smola, A.~Ahmed, V.~Josifovski,
  J.~Long, E.~J. Shekita, and B.-Y. Su, ``Scaling distributed machine learning
  with the parameter server.'' in \emph{OSDI}, vol.~14, 2014, pp. 583--598.

\bibitem{li2014communication}
M.~Li, D.~G. Andersen, A.~J. Smola, and K.~Yu, ``Communication efficient
  distributed machine learning with the parameter server,'' in \emph{Advances
  in Neural Information Processing Systems}, 2014, pp. 19--27.

\bibitem{website2013}
\BIBentryALTinterwordspacing
W.~C.~I. Dropbox~Inc, Google~Inc. Grpc. [Online]. Available:
  \url{https://grpc.io/}
\BIBentrySTDinterwordspacing

\bibitem{iandola2016firecaffe}
F.~N. Iandola, M.~W. Moskewicz, K.~Ashraf, and K.~Keutzer, ``Firecaffe:
  near-linear acceleration of deep neural network training on compute
  clusters,'' in \emph{Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition}, 2016, pp. 2592--2600.

\bibitem{dai2013petuum}
W.~Dai, J.~Wei, J.~K. Kim, S.~Lee, J.~Yin, Q.~Ho, and E.~P. Xing, ``Petuum: A
  framework for iterative-convergent distributed ml,'' 2013.

\bibitem{cui2014exploiting}
H.~Cui, J.~Cipar, Q.~Ho, J.~K. Kim, S.~Lee, A.~Kumar, J.~Wei, W.~Dai, G.~R.
  Ganger, P.~B. Gibbons \emph{et~al.}, ``Exploiting bounded staleness to speed
  up big data analytics.'' in \emph{USENIX Annual Technical Conference}, 2014,
  pp. 37--48.

\bibitem{cui2014big}
B.~Cui, H.~Mei, and B.~C. Ooi, ``Big data: the driver for innovation in
  databases,'' \emph{National Science Review}, vol.~1, no.~1, pp. 27--30, 2014.

\bibitem{cheung2002effect}
D.~W. Cheung, S.~D. Lee, and Y.~Xiao, ``Effect of data skewness and workload
  balance in parallel data mining,'' \emph{IEEE Transactions on Knowledge and
  Data Engineering}, vol.~14, no.~3, pp. 498--514, 2002.

\bibitem{ahmed2012scalable}
A.~Ahmed, M.~Aly, J.~Gonzalez, S.~Narayanamurthy, and A.~J. Smola, ``Scalable
  inference in latent variable models,'' in \emph{Proceedings of the fifth ACM
  international conference on Web search and data mining}.\hskip 1em plus 0.5em
  minus 0.4em\relax ACM, 2012, pp. 123--132.

\bibitem{patarasuk2009bandwidth}
P.~Patarasuk and X.~Yuan, ``Bandwidth optimal all-reduce algorithms for
  clusters of workstations,'' \emph{Journal of Parallel and Distributed
  Computing}, vol.~69, no.~2, pp. 117--124, 2009.

\bibitem{baidu-research_2017}
\BIBentryALTinterwordspacing
baidu research, ``baidu-research/tensorflow-allreduce,'' Aug 2017. [Online].
  Available: \url{https://github.com/baidu-research/tensorflow-allreduce}
\BIBentrySTDinterwordspacing

\bibitem{Gabriel04openmpi:}
E.~Gabriel, G.~E. Fagg, G.~Bosilca, T.~Angskun, J.~J. Dongarra, J.~M. Squyres,
  V.~Sahay, P.~Kambadur, B.~Barrett, A.~Lumsdaine, R.~H. Castain, D.~J. Daniel,
  R.~L. Graham, and T.~S. Woodall, ``Open mpi: Goals, concept, and design of a
  next generation mpi implementation,'' in \emph{In Proceedings, 11th European
  PVM/MPI Usersâ€™ Group Meeting}, 2004, pp. 97--104.

\bibitem{uberjourney}
\BIBentryALTinterwordspacing
A.~Sergeev. An uber journey in distributed deep learning. Youtube. [Online].
  Available: \url{https://www.youtube.com/watch?v=SphfeTl70MI}
\BIBentrySTDinterwordspacing

\bibitem{MNIST}
\BIBentryALTinterwordspacing
Y.~LeCun and C.~Cortes, ``{MNIST} handwritten digit database,'' 2010. [Online].
  Available: \url{http://yann.lecun.com/exdb/mnist/}
\BIBentrySTDinterwordspacing

\bibitem{chen2015mxnet}
T.~Chen, M.~Li, Y.~Li, M.~Lin, N.~Wang, M.~Wang, T.~Xiao, B.~Xu, C.~Zhang, and
  Z.~Zhang, ``Mxnet: A flexible and efficient machine learning library for
  heterogeneous distributed systems,'' \emph{arXiv preprint arXiv:1512.01274},
  2015.

\bibitem{xing2015petuum}
E.~P. Xing, Q.~Ho, W.~Dai, J.~K. Kim, J.~Wei, S.~Lee, X.~Zheng, P.~Xie,
  A.~Kumar, and Y.~Yu, ``Petuum: A new platform for distributed machine
  learning on big data,'' \emph{IEEE Transactions on Big Data}, vol.~1, no.~2,
  pp. 49--67, 2015.

\bibitem{Spark}
C.~D{\"u}nner, T.~Parnell, K.~Atasu, M.~Sifalakis, and H.~Pozidis,
  ``Understanding and optimizing the performance of distributed machine
  learning applications on apache spark,'' \emph{arXiv preprint
  arXiv:1612.01437}, 2016.

\bibitem{hashemi2016performance}
S.~H. Hashemi, S.~A. Noghabi, W.~Gropp, and R.~H. Campbell, ``Performance
  modeling of distributed deep neural networks,'' \emph{arXiv preprint
  arXiv:1612.00521}, 2016.

\bibitem{mai2015optimizing}
L.~Mai, C.~Hong, and P.~Costa, ``Optimizing network performance in distributed
  machine learning.'' in \emph{HotCloud}, 2015.

\end{thebibliography}
