<!DOCTYPE html>
<html>

<title>PBFT</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="w3.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">

<style>
body,h1,h2,h3,h4,h5,ol,li {font-family: "Arial", sans-serif}
.scale
{
    height: 21em; 
    width: 70em;
}
div{
  text-align: justify;
} 
</style>

<body stylstclass="w3-light-grey">
<div class="w3-content" style="max-width:1400px;">

<!-- Header -->
<header class="w3-container w3-center w3-padding-32"> 
<center> <h3><b> A Summary of <a href= "http://pmg.csail.mit.edu/papers/osdi99.pdf"> PBFT:</a> Practical Byzantine Fault Tolerance</b></h3></center>
</header>

<!-- Grid -->
<div class="w3-row">

<!-- Blog entries -->
<div class="container" class="w3-col l8 s12">
  <!-- Blog entry -->
  <div class="w3-card-4 w3-margin w3-white" >
    <div class="w3-container">
      
    </div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  PBFT is underline the Hyperledger blockchain and other system in blockchain space. PBFT works on an asynchronous environment like internet and handles garbage collection, view changes and request batching. In PBFT, the author not only implemented the basic version but also provide some optimizations techniques such as reducing communication by using cryptogaphic techniques.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  The protocol guarantees safety upto F where F is the number of faulty nodes. Safety is independent of network failure and does not rely on sychrony. That is, even if the network is severely partition, namely more than F nodes are isolated (but across all partitions there are still fewer than F node failures, the protocol is still safe. However, liveness is only guaranteed with fewer than F total failures, i.e. counting both node and network failures. This means at least 2F+1 nodes must be reachable. Note that safety being independent of network failures is a strong guarantee, and it is common in most variants of PBFT.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  The algorithm starts with a client that sends a request to primary and the client should hear back directly from backups and if a client does not hear back soon enough, then client broadcasts directly to backups. The primary should set the order for every request made by the client. The protocol assumes valid signatures on all replicas and has some bounds on response times to ensure liveness still possible in the case that responses are delayed arbitrarily

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  In the normal case operation, a client sends a signed (REQUEST, op, timestamp, client) message to the primary. Then, the primary interacts with backups who eventually send response messages to the client (REPLY, current_view, timestamp, client, backup_number, response) signed by backup_number. 
  <br>
  If the client sees f+1 replies with the same timestamp, response, and valid signatures, it accepts the result. If the client times out before seeing replies, it retransmits REQUEST by broadcasting directly to all replicas. This is what helps kick start a view-change later. If the client still times out, and the client will give up. In this case, no consensus possible, and not clear if the operation succeeded.
</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  The primary interacts with backups. Three-phase operation: pre-prepare, prepare, and commit. Primary multicasts request to backups, preserving signature ((PRE-PREPARE), view, sequence_number, digest>signed by primary, message). 
  <br>
  Ordering is set by a single primary if it has not accepted prepare for the current view and sequence number before. the outcome of preprepare is that backups know they need to kibitz with each other to see if enough of them have an agreement.
</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Prepare: get replicas to make an order stable each backup multicast (PREPARE, view, sequence, digest, backup number>signed by backup number to all other replicas, and adds both pre-prepare and its sent prepare messages to log each backup accepts PREPARE messages and adds those to log too.
  <br> 
  If signatures are correct, view number matches local view, and sequence number between watermarks thus, if anybody disagrees on view, everybody will discover this predicate prepared(request, view, sequence number, replica number) true if replica (replica number) has inserted into its log.
</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Commit: make order stable across views. A replica (including primary) multicasts a commit message (COMMIT, view, sequence number, Digest(message), backup number) signed by backup number when prepared(message, view, sequence number, backup number) becomes true. Replicas accept commit messages and insert in the log provided everything matches up. 

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Garbage collection is necessary to eliminate stuff from logs after a replica is convinced that at least f+1 non-faulty replicas have executed the operation. The checkpoint is used to recover another replica. 
</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Checkpoint is a snapshot-like algorithm. At some event trigger (like sequence number = 0 mod 100) all replicas issues (CHECKPOINT, sequence number, digest, backup number)signed backup number message and sends to everybody N is the latest sequence number in the checkpoint. Everybody collects these checkpoint messages in their logs When somebody has 2f+1 of them, that person has proof.
  
  <br>
  
  Basically, checkpoints for view-change operation. Replica sends out a view-change message that contains a new view number, and proof of last stable checkpoint it knows about. Also, includes "leftover" state of prepared messages that are not in the since those are used to commit these "leftovers" need a bunch of people (2f+1) to independently decide to send out view change messages before a view change happens this prevents starvation through frequent view-change when view change is initiated, the initiator stops processing non-view change messages (view change, checkpoint, and new-view) primary for new view terminates the view change protocol.

 </p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  View-change procedure ensures progress by allowing replicas to change the leader so as to not wait indeﬁnitely for a faulty primary. Each backup starts a timer when it receives a request and stops it after the request has been executed. View-change happens when a new leader did not see all 2f+1 Prepare certificates. Neither did the other f−1 non-faulty nodes. This could happen due to network partition or effected by the faulty nodes. For those nodes, (n,m) was not prepared and the sequence number n is deemed invalid.
  <br>
  Expired timers cause the backup to suspect the leader and request a view-change. It then stops receiving normal-case messages, and multicasts VIEW-CHANGE, view + 1, sequence number, s, Client, Primary, signed by sender, reporting the sequence number of the last stable checkpoints, its proof of correctness C, and the set of messages P with sequence numbers greater than n that back up i prepared since then. 
  <br>
  When the new primary receives 2f+ 1 view-change messages, it multicasts NEW-VIEW, v+ 1, V, O, N iσp, where V is the set of 2f+ 1 valid view-change messages that primary received. Upon receiving this new-view message, replicas enter view v+ 1 and re-execute the normal-case protocol for all messages in O∪N. We have proved a critical safety property of PBFT, including its garbage collection and view change procedures, which are essential in practical protocols. However, we have not yet developed generic abstractions to speciﬁcally reason about garbage collection and view-changes, that can be reused in other protocols, which we leave as future work.
  <br>
  View-change is complex, and only triggers by non-faulty nodes whose timers expired. View-change means a normal node, whose timer has not expired, will not join the protocol unless it has sees that f+1 others have. Once enough nodes voted for view change, the new leader must decide on the latest checkpoint and ensure, among other things, that non-faulty nodes are caught up with the latest states, which may involve re-committing (prepare, commit, but no execution) previously executed requests in the new view. During view change, no requests are processed.
  <br>
  View change effectively gives a new node the power of assigning sequence number to requests. It must then ensure that the new leader does not use old sequence numbers for new requests, either deliberately or gamed into it by faulty nodes. The safety property of PBFT states that: the algorithm provides safety if all non-faulty replicas agree on the sequence numbers of requests that commit locally.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  What it implies is if (n,m) is committed at a non-faulty node in view v, the tuple (n,m′) must not be committed in view v′ > v at any other non-faulty replica. The following problem does not apply to non byzantine settings which as a consequence needs only two phases (corresponding to Pre-prepare and Prepare).
  <br>
  In view v, a non-faulty node A receives 2f+1 Prepare certificates for a sequence number n attached to message m. It correctly assumes that (n,m) is ordered the same way by f+1 non-faulty nodes, including itself.
  <br>
  The solution to this is to make sure A sees that enough other nodes also received 2f+1 Prepare certificates. Only then does it execute the request and reply to the client. More specifically, a node sends out Commit certificate once it collects 2f+1 Prepare certificates. And it considers (n,m) committed only when it receives 2f+1 Commit certificates, i.e. there are 2f+1 nodes each of which sees 2f+1 Prepare certificate. Now, during view change, the new leader cannot convince other nodes that the latest sequence is n−1, as there will be at least 1 non-faulty node joining the view change with Prepare certificate for n.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Step 1. Before view change starts, a node’s state consists of:
  Checkpoints (squares) which are persisted states. A Prepared certificate for each sequence number n greater than the lowest checkpoint. Some of these sequence numbers have committed (if 2f+1 have broadcast Commit message), some have not. Regardless, the same number will not be assigned (and eventually committed) in the new view with a different message.
  <br>
  Invalid certificate (grey circle) for sequence number are Pre-prepared but no matching 2f+1 Prepare certificates. This may happen due to network failures or the leader being faulty (it sends conflicting Pre-prepare messages to different nodes).

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Step 1.5. Each node that wants to trigger view change broadcasts (VC,..,n,C,P) where n is the sequence number of its latest checkpoint, C is the checkpoint certificates (containing f+1 certificates from other replicas), and P is the Prepared certificate containing 2f+1 Prepare messages received from other replicas (and one from itself).
  <br>
  The new primary waits for 2f+1 of VC messages, which makes sure that committed sequence numbers from previous views are included in at least one prepared certificate in one VC message. A sequence n is committed at a replica if it the replica has proof that at least 2f+1 replicas have the corresponding prepared certificate. 
  <br>
  Quorum intersection implies at least 1 non-faulty node is included in the {VC} set received by the new primary, there is at least one VC message whose P contains a prepared certificate of n. This way, any prepared sequence numbers in the previous view will be visible to replicas in the new view, meaning that the same sequence number will not be re-used in the new view.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Step 2. The new primary selects the largest checkpoint sequence n from {VC}. This checkpoint is stable (stable checkpoints are discussed in the next section), thus it can be reliably fetched from the network.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Step 3. Starting from n, the primary identifies the largest sequence number H from the previous view that has a Prepare certificate, i.e. H is included in the P component of a VC message. If there is a Prepared certificate for (s,m), start consensus for (s,m) again by broadcasting Pre-Prepare message for the tuple. This is to advance the sequence number in the new view past ones that have been (or are going to be) committed in the previous view. In effect, the replicas go through Pre-prepare, Prepare, Commit phase again for (s,n), even though the replicas remember what they executed and ignore re-execution. They also remember if they have sent replies back to the client, and do not do it again in the new view.
  <br>
  If there is no Prepared certificate for s, it becomes a hole in the sequence number line. The primary fills this hole with a no-op request. Recall that holes happened when the requests are not prepared successfully due to network partition or the primary being faulty.
  <br>
  The collection of messages (step 1.5) and re-execution of previous requests (step 3) are communication bound. View change is expensive and to be avoid as much as possible. Nevertheless, it is essential to guarantee liveness, and completing it quickly is so important that the replicas stop accepting non-view-change related messages during the protocol.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  The role of prepare certificate P during view change. Recall the safety property states that if (n,m) is committed locally in a view v, there will not be a committed (n,m′) in a later view v′>v for any m ≠ m. 

 </p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
   <!-- I am done editing this one -->
  Step 1-3 above guarantee this property hold. The key idea is that if (n,m) is committed in v, the primary must pick it and pre-prepare the same tuple in the new view. The view change protocol uses Prepared certificates P to make sure the primary picks (n,m). But, is P necessary, in other words, can the primary uses Commit certificate instead? If not, does the primary only pick the sequence that was committed locally at one replica?

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Can the primary use commit certificate during view change? Instead of P in the view change message, suppose every replica include C, their Commit certificate. Next, the new primary only choose (n,m) if it is included in one Commit certificate, and null otherwise. 

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Three scenarios arise: (n,m) has not been committed at any replica in the previous view. The primary will pick (n,null). No safety violation happens here, but m is lost. The client may eventually time out and resend it, which is bad for liveness (though not a violation of liveness). (n,m) was committed at some replicas whose view change message reach the new primary. In this case, the primary will pick it, and safety is ensured. The adversary cannot fake certificate content, therefore one certificate is sufficient proof that the tuple was committed. (n,m) was committed at some replicas whose view changes message do not reach the new primary. In this case, the primary will pick (n,null) for the new view, therefore violating safety. So it does seem that Prepared certificate is necessary to achieve safety. Which then brings us to the next question.
</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Does new primary pick only the sequence that was committed? Or, does it pick (n,m) if and only if (n,m) was committed? The primary picks any (n,m) that was in a Prepared certificate, there are two cases: (n,m) was committed: then it is safe. (n,m) was not committed. This happens, for example, when a node A received 2f+1 Prepare certificates and sent out Commit message, but network faults occurred that cause all other nodes not receiving enough Prepare messages to form Prepared certificate. The primary picks the same tuple as in the previous view, therefore it is safe. Similarly, when the primary picks (n,null), there are two cases: n was not successfully prepared. It was prepared by some nodes whose view change messages do not reach the new primary. Regardless of what the primary picks, the worst it could happen is that a message prepared in a previous view is re-prepared again with a different sequence number in the new view. However, it happens only when the message was not committed, i.e. safety is guaranteed.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Signatures vs. MAC:<br> 
  signatures are generated with a private key and verified with a public key. In contrast, MACs are generated using a shared secret. From a system’s perspective, they offer different properties: signatures are often longer (RSA, DSA) and expensive to compute. MACs are cheap. Signatures are publicly verifiable, MACs are only verifiable with the secret key. An important consequence is that signatures are irrefutable: once Alice signed a message, anyone will be convinced of its authenticity. On the other hand, MAC cannot be used to prove authenticity, i.e. once Alice generated a MAC for a message with Bob, Bob cannot use this as evidence that Alice has signed. A third party, Veronica, cannot verify that the MAC is correct, since he does not know the secret key. Even Bob shares with Veronica the secret, the latter still does not know whether the MAC is created by Bob or by Alice. The first property was what came to my mind when reading a passage in the original paper about alternative implementations of the protocol. I had quickly dismissed it as merely an implementation choice. I had a second chance of re-examining it when reading Byzcoin (by Kogias et. al. in Usenix Security 2016), which has one optimization to reduce communication overhead by replacing MACs with signatures. But not until reading Castro’s PhD thesis did I learn the far-reaching implication of the choice between MACs and signatures. No surprise that the thesis devotes individual section for each of them.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  High-level idea. During view change of PBFT, the new leader has to prove to other nodes that it has the right to trigger view change, by bundling view-change requests from other nodes. Furthermore, a node must prove to another that it has a certain checkpoint, or is able to fetch another checkpoint. This proof is needed for selecting an initial state from which the new view begins. How to generate such proofs? Using signatures, a node can collect a set of signed statements from others, i.e. certificates containing the needed information. It can then show to another as proof that other nodes have had the information. Since there is no way to forge signatures, these certificates are sufficient. However, with MAC, the verifying node relies on non-faulty nodes to broadcast the same information. That is, rather than A present to B a certificate as proof of authenticity, it counts on B to seeks verifying information from other nodes. The challenge is for A to be certain that B will succeed in obtaining such proof.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  Stable checkpoints: checkpoints are essentially persisted states. They serve two roles: garbage collection and state transfer. When a checkpoint is adequately replicated, previous states including messages and certificates can be discarded. During a view change, a checkpoint is selected as the most recent states most nodes agree on. From that, some nodes need to catch up (by transferring states from another node) and some requests need to be re-committed in a new view.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  To fulfill its roles, a node collects checkpoint certificates from others, and considers a checkpoint c stable when it is sufficiently replicated and can be reliably retrieved by any other node. With signatures, f+1 certificates are sufficient to form a stable certificate. The nodes generating these certificates cannot deny that they have the checkpoint, because they have signed them. And f+1 ensure that at least one of them is reachable.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  With MACs, 2f+1 certificates (MACs) are needed to form a stable certificate. This is to ensure at least f+1 non-faulty nodes store the checkpoint, and later on these f+1 can send the same information to the verifier to prove the existence of the checkpoint. Suppose Alice collects only f+1 certificates (one from itself) before considering the checkpoint c stable. The other f can be controlled by the adversary. Later on, Bob wants to retrieve c, but the adversary present f invalid checkpoints. There is no way for Bob to determine which one is correct (a similar problem in quorum systems without data authenticate — see the post on quorum). Then, the checkpoint is essentially lost. Any number smaller than 2f+1 is not enough, because the adversary, controlling f certificates, can convince the verifier to accept invalid checkpoint.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
  View change: This excerpt from the paper is an understatement: We were able to retain the same communication structure during normal case operation and garbage collection at the expense of significant and subtle changes to the view change protocol.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">

  A key step during a view change is for the primary to reliably pick messages from the previous view to pre-prepare again in the new view. Signatures provide a means to verify that (n,m) was truly prepared by some replicas, therefore if it was committed in the previous view it will be picked. Without signatures, verification is more complex. Each replica must verify by itself by collecting f+1 messages from other replicas, as opposed to checking one signed message.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">


  The new view change proceeds as follows:
  Step 1. The replica r collects message into P Set and Q Set, which are then included in the view change message: PSet contains tuple (n,m,v) that are prepared at r at view v. QSet contains tuple (n,m,v) that are pre-prepared at r at view v. Note the difference between (PSet,QSet) and P. The former contains local information; the latter bundle certificates from remote replicas as well. Without signatures, remote messages are of no use.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">

  Step 1.5. The primary waits for 2f+1 view change messages. Previously, this quorum ensures that committed messages are included in at least one view change message. Here, it also ensures that there will be a weak certificate of f+1 messages from the non-faulty nodes which can serve as proof that some information is stored.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">

  Step 2. The primary picks the starting state of the new view by choosing a stable checkpoint with sequence s which is backed by f+1 view change messages.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">

  Step 3. The primary picks message to pre-prepare as follows: It picks (n,m) if n has not been garbage collected by 2f+1 replicas, it is included in a PSet, and there are f+1 matching QSet messages. It picks (n,null) if there are 2f+1 PSet messages that do not have n.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">

  Step 3 is very confusing, and I am still unsure if I understood the reason behind it. Here is what I think I understand, based on an example illustrated in the figure below. In this example, Byzantine nodes are marked red, non-faulty nodes that took part in the commit phase are blue, and the other non-faulty, partitioned nodes are in grey. Let’s first see why step 3 is safe. If (n,m) is committed in view v, it will be picked by the primary. (n,m) was committed at A, thus there are at least one (n,m) is seen by the primary during view change. But the adversary can also present (n,m′) during view change. How do the primary pick the correct one? The condition of f+1 matching QSet message solve ensures that (n,m′) will not be picked. In the figure, the first view change cannot pick (n,m) either. There may be many view changes before it can be satisfied by a quorum of 2f+1 non-faulty nodes. Notice that with only 2f+1, the primary may not be able to pick any message to prepare — both conditions are not met. As the result, the view change may time out. The paper makes the assumption that eventually a quorum of non-faulty nodes is available, then the primary will pick either (n,m) or (n,null).

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">

  Recall that during view change protocol with signatures, the primary also pick (n,m) that was prepared at some replicas but not yet committed. The same happens here: (n,m) was prepared at A, meaning that it must be pre-prepared at f+1 honest nodes. Note that (n,m) only have to appear once in 2f+1 view change messages, similar to the protocol with signatures. f+1 matching QSet will eventually be satisfied, maybe after many view changes. Then, (n,m) is picked for the new view. It also happens that a prepared message in view v may be lost. Suppose A receives a Prepared certificate for (n,m), but all others do not. During view change A was isolated, and there was no (n,m) found in PSet. The primary then picks (n,null).

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">


  In summary, view change without MACs is safe. It may be slower in making progress, but it has the same eventual liveness as with signatures.<br>

  Discrepancy notes. Even Castro’s thesis doesn’t contain a formal model and proof for the MAC-based protocol. Hyperledger’s implementation of view change follows the MAC-based protocol, except that view change messages are signed, obviating the need for each replica to collect view change messages directly from each other. In other words, the primary can collect all 2f+1 view change messages and bundle them when sending new view messages to replica.

</p></div>


<div class="w3-container"><p style="color:black;font-size:20px;">

   This two aspects improve their performance.
   <br>
   1. I am not sure the assumption of independent node failures is good or not. They provided a lot of ways to achieve this assumption, but, from practical perspective, homogenous services may be easy to managed and less costly. If the system need to follow their assumption, I do not think the system can be scalable.
   <br>
   2. Also, in their algorithm, the client needs to wait for (f+1) replies from different replicas with the same result, which is necessary for correctness. However, this also means the client can know the actual f value for the system. How about some malicious clients attack (f+1) replicas. So, I think all the core system parameters should be hidden from the client in order to protect the system.

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
    <h4>References</h4>
    <footer>
        [1] Castro-Liskov <cite><a href="https://www.youtube.com/watch?v=Q0xYCN-rvUs"><i>PBFT </i></a> Lecture at MicroSoft Research</cite>.
    </footer>
    <footer>
        [2] CS 739 <cite><a href="http://pages.cs.wisc.edu/~swift/classes/cs739-fa14/blog/2014/10/practical_byzantine_fault_tole.html"><i>PBFT </i></a> by Michael Swift</cite>.
    </footer>

</p></div>

<div class="w3-container"><p style="color:black;font-size:20px;">
  <!-- I am done editing this one -->
 Benchmarks:<br>
 ran Andrew benchmark with a single client
 means that each operation does full RTT before next is issued. It means that server is underutilized and overhead of crypto is not included. Only seeing effect of extra round-trip of protocol.

</p></div>

</p></div>

</div> <hr>

<!-- END BLOG ENTRIES -->
</div>

<!-- END GRID -->
</div><br>

<!-- END w3-content -->
</div>

<!-- Footer -->
<footer class="w3-container w3-dark-grey w3-padding-32 w3-margin-top">
  <center><a href="/webpage/MyBlog/Blog.html" class="w3-button w3-padding-large w3-white w3-border" class="w3-button w3-black w3-padding-large w3-margin-bottom">Main Blog</a></button></center>
</footer>
</body>
</html>